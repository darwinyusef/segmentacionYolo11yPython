{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf48190",
   "metadata": {},
   "source": [
    "## Umbralización para obtener detección con segmentación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a258530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (8.3.103)\n",
      "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from ultralytics) (2.1.1)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from ultralytics) (3.10.1)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from ultralytics) (11.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from ultralytics) (1.15.2)\n",
      "Requirement already satisfied: torch>=1.8.0 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from ultralytics) (2.6.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from ultralytics) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from ultralytics) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
      "Requirement already satisfied: filelock in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.13.1)\n",
      "Requirement already satisfied: networkx in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: colorama in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in i:\\documents\\uniria\\percepcióncomputacional\\laboratorio2\\segyolo\\venv\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74047a46",
   "metadata": {},
   "source": [
    "# Análisis de Movimiento en Video con Segmentación y Mapas de Calor\n",
    "\n",
    "Este proyecto implementa una técnica para analizar el movimiento en secuencias de video, combinando sustracción de fondo y segmentación de instancias mediante el modelo YOLOv11. El objetivo es identificar y rastrear objetos en movimiento, específicamente personas, y visualizar su trayectoria a través de un mapa de calor acumulativo.\n",
    "\n",
    "## Descripción del Código\n",
    "\n",
    "1. **Inicialización**: Se carga el modelo YOLOv11 para segmentación (`yolo11n-seg`) y se inicializa el acumulador del mapa de calor `heatmap_refined`.\n",
    "\n",
    "2. **Captura de Video**: Se procesa cada fotograma del video en un bucle continuo. Si no se puede leer un fotograma, el proceso se detiene.\n",
    "\n",
    "3. **Sustracción de Fondo**: Se aplica un sustractor de fondo para obtener una máscara binaria que resalta las áreas en movimiento en el fotograma actual.\n",
    "\n",
    "4. **Segmentación con YOLOv11**: Se utiliza el modelo YOLOv11 para realizar la segmentación de instancias en el fotograma, identificando objetos y generando máscaras para cada instancia detectada.\n",
    "\n",
    "5. **Filtrado de Personas**: Se filtran las máscaras correspondientes a la clase \"persona\" (clase 0 en COCO) y se combinan en una única máscara de segmentación.\n",
    "\n",
    "6. **Combinación de Máscaras**: Se realiza una intersección entre la máscara de movimiento y la máscara de segmentación de personas para obtener una máscara refinada que representa las áreas donde hay movimiento de personas.\n",
    "\n",
    "7. **Actualización del Mapa de Calor**: La máscara refinada se acumula en el `heatmap_refined`, que visualiza las áreas con mayor actividad a lo largo del tiempo.\n",
    "\n",
    "8. **Visualización**: Se muestran en ventanas separadas el fotograma original, la máscara de movimiento y la máscara refinada. El proceso continúa hasta que se presiona la tecla 'q'.\n",
    "\n",
    "## Contexto Teórico\n",
    "\n",
    "La combinación de sustracción de fondo y segmentación de instancias es una técnica poderosa en visión por computadora para detectar y rastrear objetos en movimiento. La sustracción de fondo permite identificar áreas de cambio en una escena estática, mientras que la segmentación de instancias, como la proporcionada por YOLOv11, permite identificar y delinear objetos específicos dentro de esas áreas. Al combinar ambas técnicas, es posible aislar y rastrear objetos de interés, como personas, en secuencias de video, facilitando aplicaciones en seguridad, análisis de comportamiento y más.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb8c5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dda8c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_path = \"park_detection.avi\"\n",
    "video_path = \"videos/people-detection.mp4\"\n",
    "models = \"yolo11n-seg.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a6f4091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrir el video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Crear sustractor de fondo\n",
    "bg_subtractor = cv2.createBackgroundSubtractorMOG2(\n",
    "    history=500,          # Número de frames usados para construir el fondo.\n",
    "    varThreshold=16,      # Sensibilidad para detectar cambios\n",
    "    detectShadows=True,   # Detección de sombras\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2533cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_refined = None\n",
    "\n",
    "# Cargar el modelo YOLOv11 para segmentación\n",
    "model = YOLO(\"yolo11n-seg\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Inicializar el acumulador del heatmap en el primer frame\n",
    "    if heatmap_refined is None:\n",
    "        heatmap_refined = np.zeros(frame.shape[:2], dtype=np.float32)\n",
    "\n",
    "    # --- Paso 1: Sustracción de Fondo ---\n",
    "    fgmask = bg_subtractor.apply(frame)\n",
    "    # Umbral para obtener una máscara binaria limpia\n",
    "    _, fgmask = cv2.threshold(fgmask, 200, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # --- Paso 2: Segmentación con YOLO ---\n",
    "    # Realizamos la detección con segmentación sobre el frame completo.\n",
    "    results = model(frame, verbose=False)[0]\n",
    "\n",
    "    # Crear una máscara vacía para acumular las segmentaciones de la clase \"persona\"\n",
    "    segmentation_mask = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    if results.masks is not None:\n",
    "        # Extraer las máscaras y las clases\n",
    "        masks = results.masks.data.cpu().numpy() if hasattr(results.masks.data, 'cpu') else results.masks.data\n",
    "        classes = results.boxes.cls.cpu().numpy() if hasattr(results.boxes.cls, 'cpu') else results.boxes.cls\n",
    "\n",
    "        for mask, cls in zip(masks, classes):\n",
    "            if int(cls) == 0:  # Filtramos detecciones de persona (en COCO, \"person\" es la clase 0)\n",
    "                mask_bin = (mask > 0.5).astype(np.uint8) * 255\n",
    "                # Redimensionar mask_bin a las dimensiones del frame (o segmentation_mask)\n",
    "                mask_bin_resized = cv2.resize(mask_bin, (segmentation_mask.shape[1], segmentation_mask.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "                segmentation_mask = cv2.bitwise_or(segmentation_mask, mask_bin_resized)\n",
    "\n",
    "    # --- Paso 3: Combinación de Máscaras ---\n",
    "    # Se realiza una intersección entre la máscara de movimiento y la máscara de segmentación de personas\n",
    "    refined_mask = cv2.bitwise_and(fgmask, segmentation_mask)\n",
    "\n",
    "    # Acumulamos la máscara refinada en el heatmap\n",
    "    heatmap_refined = cv2.add(heatmap_refined, refined_mask.astype(np.float32))\n",
    "\n",
    "    # Visualización intermedia\n",
    "    cv2.imshow(\"Frame Original\", frame)\n",
    "    cv2.imshow(\"Mascara Movimiento (FG)\", fgmask)\n",
    "    # cv2.imshow(\"Mascara Segmentacion (Personas)\", segmentation_mask)\n",
    "    cv2.imshow(\"Mascara Refinada\", refined_mask)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
