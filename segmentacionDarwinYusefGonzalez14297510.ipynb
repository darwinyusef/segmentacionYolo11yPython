{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "_NkPebJAd3Cu",
        "6NKBSD3ejxY-",
        "89GK2IBuoPD_",
        "aGKESt0zvINm"
      ],
      "authorship_tag": "ABX9TyMmKMddJjQq6ZaGAorgFrLH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darwinyusef/segmentacionYolo11yPython/blob/master/segmentacionDarwinYusefGonzalez14297510.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trabajo Actividad 2 Evaluación de la segmentación\n",
        "\n",
        "# PRESENTACIÓN\n",
        "+ ### DARWIN YUSEF GONZALEZ TRIANA\n",
        "+ ### C.C. 14297510\n",
        "+ ### © Universidad Internacional de La Rioja (UNIR)\n",
        "+ ### INVESTIGACIÓN EN IA\n",
        "\n",
        "\n",
        "![Presentación](https://raw.githubusercontent.com/darwinyusef/20exHuggingFacePytorchTensorFlowSklearn/refs/heads/master/skitimage/intensity/presentacion.png)\n",
        "\n",
        "## Objetivos\n",
        "\n",
        "El objetivo de este trabajo es aprender a construir y evaluar el rendimiento de uno o más segmentadores. Esta actividad permitirá consolidar los conceptos sobre segmentación de imágenes aprendidos.\n",
        "\n",
        "## Descripción\n",
        "\n",
        "Este trabajo se entrega de forma individual. En él nos vamos a enfrentar a un verdadero problema de segmentación. La segmentación, como se ha visto, consiste en detectar regiones homogéneas y aislar/detectar objetos dentro de una imagen. Estas regiones habitualmente corresponden a los objetos que se están queriendo identificar.\n",
        "\n",
        "Existen muchas maneras de enfocar este problema y puedes hacer uso de las técnicas de segmentación que consideres para resolverlo. Una vez elegidas estas técnicas, debes evaluar su rendimiento frente a imágenes de ground truth. En caso de que se utilicen partes de un software existente, deberá referenciarse la fuente. Debes mostrar en pantalla los resultados de los principales pasos."
      ],
      "metadata": {
        "id": "uAaid5KXx2G-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tZlrksEpIOL"
      },
      "outputs": [],
      "source": [
        "# lab ex 1\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "# Cargar la imagen usando Pillow\n",
        "from PIL import Image\n",
        "\n",
        "# lab ex 2 clusters\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "from sklearn.preprocessing import normalize\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cargar imagen\n",
        "!wget https://raw.githubusercontent.com/darwinyusef/UsaHousingLab/refs/heads/master/image.png\n",
        "\n",
        "# cargar roma\n",
        "!wget https://raw.githubusercontent.com/darwinyusef/UsaHousingLab/refs/heads/master/roma.png\n",
        "\n",
        "# cargar calle\n",
        "!wget https://raw.githubusercontent.com/darwinyusef/UsaHousingLab/refs/heads/master/calle.png"
      ],
      "metadata": {
        "id": "q6Fj81JYaEmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Segmentación de colores sobre centroides basada en K-MEANS\n",
        "\n",
        "Segmentación de colores sobre centroides basada en K-Means\" es una técnica de procesamiento de imágenes que utiliza el algoritmo de clustering K-Means para agrupar los colores de los píxeles en una imagen. El proceso implica los siguientes pasos:\n",
        "\n",
        "En esencia, esta técnica reduce la paleta de colores de una imagen agrupando colores similares y reemplazándolos por sus colores promedio (centroides). Es útil para simplificar imágenes, reducir la cantidad de colores necesarios para su representación y como un paso previo en algunas tareas de análisis de imágenes."
      ],
      "metadata": {
        "id": "_NkPebJAd3Cu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"/content/image.png\""
      ],
      "metadata": {
        "id": "O25HzJZZWM59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Imagen cargada: {image_path}\")\n",
        "\n",
        "# Cargar la imagen usando Pillow\n",
        "original_image = Image.open(image_path)\n",
        "plt.imshow(original_image)\n",
        "plt.title(\"Imagen Original\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xfrsi0--paId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir la imagen a un array numpy\n",
        "image_array = np.array(original_image)\n",
        "\n",
        "# Obtener las dimensiones de la imagen\n",
        "height, width, channels = image_array.shape\n",
        "\n",
        "# Reformatear la imagen para que cada píxel sea una fila\n",
        "reshaped_image = image_array.reshape(height * width, channels)\n",
        "\n",
        "print(f\"Forma del array de la imagen original: {image_array.shape}\")\n",
        "print(f\"Forma del array de la imagen reformateada: {reshaped_image.shape}\")"
      ],
      "metadata": {
        "id": "2i-GAHj3p-8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Definir el número de clusters (segmentos)\n",
        "n_clusters = 4 # Puedes experimentar con diferentes valores hasta un maximo de 8"
      ],
      "metadata": {
        "id": "gEls35Pmvzzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Inicializar y entrenar el modelo K-means\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=10) # n_init para evitar problemas de convergencia\n",
        "kmeans.fit(reshaped_image)\n",
        "\n",
        "# Obtener las etiquetas de los clusters para cada píxel\n",
        "cluster_labels = kmeans.predict(reshaped_image)\n",
        "\n",
        "# Obtener los centros de los clusters (representan los colores promedio de cada segmento)\n",
        "cluster_centers = kmeans.cluster_centers_"
      ],
      "metadata": {
        "id": "w6PNc8u_qC6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Inicia el entrenamiento no supervisado\n",
        "import cv2\n",
        "\n",
        "original_image = Image.open(image_path)\n",
        "image_array = np.array(original_image)\n",
        "height, width, channels = image_array.shape\n",
        "reshaped_image = image_array.reshape(height * width, channels)\n",
        "\n",
        "# Inicializar y entrenar el modelo K-means\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=10)\n",
        "# aqui corremos el entrenados\n",
        "kmeans.fit(reshaped_image)\n",
        "# obtenemos las prediciones y el cluster_centers\n",
        "cluster_labels = kmeans.predict(reshaped_image)\n",
        "cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "# Crear la imagen segmentada basada en los centros de los clusters\n",
        "segmented_image_array = cluster_centers[cluster_labels].reshape(height, width, channels).astype(np.uint8)\n",
        "# aparece la imagen segmentada (resultado)\n",
        "segmented_image = Image.fromarray(segmented_image_array)\n",
        "\n"
      ],
      "metadata": {
        "id": "wBwjrOxZwjnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Grafica los segmentos y su respectivo histograma\n",
        "# @markdown Es crucial entender que este codigo nos lleva a poder observar el cambio tan radical en comparación entre la imagen y el\n",
        "# @markdown *la imagen segmentada*  nos permite comparar que efectivamente el listado de centroides de busqueda por k-means divide la imagen en colores especificos agrupando los mismos a traves del entrenamiento no supervisado\n",
        "\n",
        "# ------------------- Histograma de la Imagen Original -------------------\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(original_image)\n",
        "plt.title(\"Imagen Original\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "color = ('b', 'g', 'r') # El orden de los canales en OpenCV es BGR por defecto simplemente los cambio\n",
        "# aplico la funcion del historigrama\n",
        "for i, col in enumerate(color):\n",
        "    histogram = cv2.calcHist([image_array], [i], None, [256], [0, 256])\n",
        "    plt.plot(histogram, color=col)\n",
        "    plt.xlim([0, 256])\n",
        "plt.title(\"Histograma de Color (Original)\")\n",
        "plt.xlabel(\"Valor del Píxel\")\n",
        "plt.ylabel(\"Frecuencia\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ------------------- Histograma de la Imagen Segmentada -------------------\n",
        "segmented_image_array_cv2 = np.array(segmented_image) # Convertir a array para OpenCV\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(segmented_image)\n",
        "plt.title(f\"Imagen Segmentada con {n_clusters} Clusters\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "color = ('b', 'g', 'r')\n",
        "for i, col in enumerate(color):\n",
        "    histogram = cv2.calcHist([segmented_image_array_cv2], [i], None, [256], [0, 256])\n",
        "    plt.plot(histogram, color=col)\n",
        "    plt.xlim([0, 256])\n",
        "plt.title(\"Histograma de Color (Segmentada)\")\n",
        "plt.xlabel(\"Valor del Píxel\")\n",
        "plt.ylabel(\"Frecuencia\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IdU2OITioue-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exploración de la imagen creando una nueva a traves de reshape, sus cluster_labels, y sus clustes centros\n",
        "# @markdown  De tal manera que obtenemos de los clusters los centros y sus respectivas agrupaciones ahora podemos verlo a traves de sus canales(channels) por medio de reshape\n",
        "\n",
        "# Crear una nueva imagen basada en las etiquetas de los clusters y los centros\n",
        "segmented_image_array = cluster_centers[cluster_labels].reshape(height, width, channels).astype(np.uint8)\n",
        "print(channels)\n",
        "# Convertir el array numpy a un objeto de imagen PIL\n",
        "segmented_image = Image.fromarray(segmented_image_array)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "\n",
        "\n",
        "axes[0].imshow(original_image)\n",
        "axes[0].set_title(\"Imagen Original\")\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(segmented_image)\n",
        "axes[1].set_title(f\"Imagen Segmentada ({n_clusters} Clusters)\")\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_JCmNgkyqIiW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A la hora de hacer la revisión de la segmentación por kmens se supone que esta debe reagrupar los elementos, aunque la segmentación contiee muchos puntos del mismo array se puede observar que los mismos se repiten mucho ejm 102/136/135 estos se repiten con base a la cantidad de clusters tiene la imagen"
      ],
      "metadata": {
        "id": "cSirEoXLb1aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown  [[[102 136 135]   [102 136 135]   [102 136 135]   ...\n",
        "# print(segmented_image_array)"
      ],
      "metadata": {
        "id": "jmqsryswbxEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Suponiendo que 'original_image_array' es tu imagen original (antes de K-Means)\n",
        "\n",
        "segmented_image_array = np.array(segmented_image)\n",
        "num_pixels = segmented_image_array.shape[0] * segmented_image_array.shape[1]\n",
        "# y 'labels_array' contiene la etiqueta del clúster asignada a cada píxel.\n",
        "\n",
        "labels_array = np.random.randint(0, 3, size=(segmented_image_array.shape[0], segmented_image_array.shape[1])) # Ejemplo con 3 clústeres\n",
        "num_clusters = np.max(labels_array) + 1\n",
        "\n",
        "# Las dimensiones de 'labels_array' deberían ser (altura, ancho) de la imagen.\n",
        "pixels = segmented_image_array.reshape(-1, 3) # Use the NumPy array here as well\n",
        "flat_labels = labels_array.flatten()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MhRRoSAHa0I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Grafia de visualización 2D de numpy array [102 136 135]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(pixels[:, 0], pixels[:, 1], c=flat_labels, cmap='viridis', s=5) # Usamos Rojo y Verde para el gráfico 2D\n",
        "\n",
        "# Añadir etiquetas y título\n",
        "plt.xlabel('Componente Rojo')\n",
        "plt.ylabel('Componente Verde')\n",
        "plt.title('Visualización de Clústeres K-Means (Proyección 2D)')\n",
        "\n",
        "# Crear una leyenda para los clústeres\n",
        "legend = plt.legend(*scatter.legend_elements(), title=\"Clústeres\")\n",
        "plt.gca().add_artist(legend)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "x_rrPTYia7v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Es neceario observar que los clusters poseen una variación 3d ya que la posición z tambien se encuentra posicionada por esto la evaluación puede durar 40 seg"
      ],
      "metadata": {
        "id": "TPiMqHUocVAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "scatter = ax.scatter(pixels[:, 0], pixels[:, 1], pixels[:, 2], c=flat_labels, cmap='viridis', s=20)\n",
        "\n",
        "ax.set_xlabel('Componente Rojo')\n",
        "ax.set_ylabel('Componente Verde')\n",
        "ax.set_zlabel('Componente Azul')\n",
        "ax.set_title('Visualización de Clústeres K-Means (3D)')\n",
        "\n",
        "legend = ax.legend(*scatter.legend_elements(), title=\"Clústeres\")\n",
        "ax.add_artist(legend)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "92zx2RgUbdmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explicación de la segmentación realizada a través del entrenamiento de modelo K-means\n",
        "\n",
        "Este código realiza una segmentación de color de una imagen utilizando el algoritmo K-means. de mi parte realice toda la consuta ya que desde el inicio contamos con la respectiva segmentación\n",
        "\n",
        "+ Carga y Preparación de la Imagen: La imagen original se abre y se convierte en un array NumPy. Luego, se reshapea para que cada fila represente un píxel y las columnas representen los canales de color (e.g., Rojo, Verde, Azul).\n",
        "\n",
        "+ Inicialización y Entrenamiento de K-means: Se inicializa un modelo K-means con un número específico de clusters (n_clusters). Este modelo se entrena con los datos de los píxeles reshapeados para agrupar píxeles de colores similares en clusters. Se realizan múltiples inicializaciones (n_init=10) para obtener un mejor resultado.\n",
        "\n",
        "+ Predicción de Clusters: Una vez entrenado, el modelo K-means predice a qué cluster pertenece cada píxel de la imagen. Estas etiquetas de cluster se almacenan en cluster_labels.\n",
        "\n",
        "+ Definición de Paleta de Colores: Se define una lista de colores personalizados. El código verifica que haya suficientes colores definidos para el número de clusters especificado.\n",
        "\n",
        "+ Creación de la Imagen Segmentada: Se crea una nueva imagen (array) con las mismas dimensiones que la original. A cada píxel de esta nueva imagen se le asigna el color correspondiente al cluster al que fue asignado por K-means, utilizando la paleta de colores definida.\n",
        "\n",
        "+ Visualización: Finalmente, la imagen segmentada con los colores personalizados se convierte de nuevo en un objeto Image y se muestra utilizando matplotlib. El título indica el número de clusters utilizados."
      ],
      "metadata": {
        "id": "UMC9bwr3cp9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title la segmentación anterior la pintamos con un array de colores que permiten diferir la segmentación de las imagenes\n",
        "\n",
        "original_image = Image.open(image_path)\n",
        "image_array = np.array(original_image)\n",
        "height, width, channels = image_array.shape\n",
        "reshaped_image = image_array.reshape(height * width, channels)\n",
        "\n",
        "\n",
        "# Inicializar y entrenar el modelo K-means\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=10)\n",
        "kmeans.fit(reshaped_image)\n",
        "cluster_labels = kmeans.predict(reshaped_image)\n",
        "\n",
        "# Definir una paleta de colores para los clusters (ejemplo con algunos colores)\n",
        "colors = [\n",
        "    [255, 0, 0],\n",
        "    [0, 255, 0],\n",
        "    [0, 0, 255],\n",
        "    [255, 255, 0],\n",
        "    [255, 0, 255],\n",
        "    [255, 100, 255],\n",
        "    [100, 100, 0],\n",
        "    [100, 50, 50]\n",
        "]\n",
        "\n",
        "# Asegurarse de que haya suficientes colores para todos los clusters\n",
        "if n_clusters > len(colors):\n",
        "    raise ValueError(f\"Se necesitan al menos {n_clusters} colores en la paleta.\")\n",
        "\n",
        "# Crear la imagen segmentada con colores personalizados\n",
        "segmented_image_array_rainbow = np.zeros((height * width, channels), dtype=np.uint8)\n",
        "for i in range(n_clusters):\n",
        "    segmented_image_array_rainbow[cluster_labels == i] = colors[i]\n",
        "\n",
        "segmented_image_array_rainbow = segmented_image_array_rainbow.reshape(height, width, channels)\n",
        "segmented_image_rainbow = Image.fromarray(segmented_image_array_rainbow)\n",
        "\n",
        "# Mostrar la imagen segmentada con colores personalizados\n",
        "plt.imshow(segmented_image_rainbow)\n",
        "plt.title(f\"Imagen Segmentada con {n_clusters} Clusters con visualización variada por color\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uxkYjVl-taov",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se podría usar para extraer las paletas de colores dominantes de imágenes o diseños. Creación de Muestrarios de Color: Agrupar colores similares puede ayudar a crear los siguientes elementos a continuación.\n",
        "\n",
        "1. Edición y Mejora de Imágenes:\n",
        "2. Análisis de Imágenes y Visión por Computadora:\n",
        "3. Aplicaciones Médicas:\n",
        "4. Teledetección y Geografía:\n",
        "5. Control de Calidad Industrial:\n",
        "6. Marketing y Diseño:\n",
        "\n"
      ],
      "metadata": {
        "id": "9QWlUSSmhl4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Exploración de la Umbralización para segmentación a traves del uso de\n",
        "\n",
        "+ THRESH_BINARY\n",
        "+ THRESH_BINARY_INV\n",
        "+ THRESH_BINARY\n",
        "+ THRESH_OTSU\n",
        "+ THRESH_BINARY\n",
        "+ THRESH_BINARY"
      ],
      "metadata": {
        "id": "6NKBSD3ejxY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "m4B2t5d4iGGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '/content/roma.png'\n",
        "img = cv2.imread(image_path)\n",
        "\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "axes[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) # Display in RGB for matplotlib\n",
        "axes[0].set_title(\"Imagen Original\")\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(gray, cmap='gray') # Specify grayscale colormap\n",
        "axes[1].set_title(\"Imagen en Escala de Grises\")\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ScYbZ0sjvBRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicar diferentes técnicas de umbralización\n",
        "\n",
        "La umbralización es un proceso para segmentar una imagen, separando objetos de interés del fondo basándose en la intensidad de los píxeles. El código aplica varias técnicas:\n",
        "Umbralización binaria simple (cv2.THRESH_BINARY y cv2.THRESH_BINARY_INV): Establece los píxeles a un valor blanco o negro dependiendo de si su intensidad está por encima o por debajo de un umbral fijo (thresh_value). La versión INV invierte esta asignación.\n",
        "Umbralización de Otsu (cv2.THRESH_BINARY + cv2.THRESH_OTSU): Un método automático para encontrar un valor de umbral óptimo basado en el histograma de la imagen.\n",
        "Umbralización adaptativa (cv2.ADAPTIVE_THRESH_MEAN_C y cv2.ADAPTIVE_THRESH_GAUSSIAN_C): En lugar de un umbral global, calcula un umbral local para cada píxel basándose en la intensidad de los píxeles vecinos. Los métodos MEAN_C y GAUSSIAN_C utilizan diferentes formas de calcular este umbral local (la media o una media ponderada por una función gaussiana)."
      ],
      "metadata": {
        "id": "eLDC6YbQlWhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Explorando una gama basica de Umbralización binaria para darle uso a procesos de segmentación\n",
        "\n",
        "\n",
        "image_path = '/content/roma.png'\n",
        "img = cv2.imread(image_path)\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Umbralización binaria simple\n",
        "thresh_value = 128\n",
        "_, thresh_binary = cv2.threshold(gray, thresh_value, 255, cv2.THRESH_BINARY)\n",
        "_, thresh_binary_inv = cv2.threshold(gray, thresh_value, 255, cv2.THRESH_BINARY_INV)\n",
        "_, thresh_otsu = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "thresh_adapt_mean = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
        "thresh_adapt_gauss = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
        "\n",
        "# Mostrar imagen original\n",
        "axes[0, 0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "axes[0, 0].set_title(\"Imagen Original\")\n",
        "axes[0, 0].axis('off')\n",
        "\n",
        "# Mostrar imagen en escala de grises\n",
        "axes[0, 1].imshow(gray, cmap='gray')\n",
        "axes[0, 1].set_title(\"Imagen en Escala de Grises\")\n",
        "axes[0, 1].axis('off')\n",
        "\n",
        "# Mostrar umbralización binaria simple\n",
        "axes[0, 2].imshow(thresh_binary, cmap='gray')\n",
        "axes[0, 2].set_title(f\"Umbral Binario ({thresh_value})\")\n",
        "axes[0, 2].axis('off')\n",
        "\n",
        "# Mostrar umbralización binaria inversa\n",
        "axes[1, 0].imshow(thresh_binary_inv, cmap='gray')\n",
        "axes[1, 0].set_title(f\"Umbral Binario Inverso ({thresh_value})\")\n",
        "axes[1, 0].axis('off')\n",
        "\n",
        "# Mostrar umbralización de Otsu\n",
        "axes[1, 1].imshow(thresh_adapt_gauss, cmap='gray')\n",
        "axes[1, 1].set_title(f\"Umbral de Otsu\")\n",
        "axes[1, 1].axis('off')\n",
        "\n",
        "# Mostrar umbralización adaptativa (Media)\n",
        "axes[1, 2].imshow(thresh_adapt_mean, cmap='gray')\n",
        "axes[1, 2].set_title(\"Umbral Adaptativo (Media)\")\n",
        "axes[1, 2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yD55hSfNw2ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "H7qX2oIGmZou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Segmentación de imagenes a traves de selección gama de Umbralización  y / o agrupamiento de contornos\n",
        "###### (TENER EN CUENTA EL TEMA ANTERIOR)\n",
        "Este código realiza una segmentación de imagen utilizando el algoritmo SLIC (Simple Linear Iterative Clustering). Primero, convierte la imagen a formato RGB y luego a valores flotantes. SLIC agrupa píxeles similares en superpíxeles compactos. Las regiones segmentadas se pintan con colores promedio, y luego se dibujan bordes azules para visualizarlos claramente. El resultado final es una imagen donde los objetos o regiones con características similares están delimitados por bordes de color. Esto es útil para simplificar el análisis de imágenes, identificar objetos y reducir la complejidad para tareas posteriores como el reconocimiento."
      ],
      "metadata": {
        "id": "89GK2IBuoPD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.segmentation import slic, mark_boundaries\n",
        "from skimage.util import img_as_float\n",
        "from skimage.segmentation import slic, mark_boundaries\n",
        "from skimage.color import label2rgb\n",
        "from skimage.util import img_as_float\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "kxuxmwsxns52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Encontrar contornos, aqui solo usamos TRESH_BINARY PERO COMO SE OBSERVO ANTES SE PUEDE AMPLIAR MUCHO MÁS\n",
        "_, thresh = cv2.threshold(gray, thresh_value, 255, cv2.THRESH_BINARY) # <--- SOLO DEBES CAMBIAR EL TIPO DE UMBRALIZACIÓN PARA SELECCIÓN DE CONTORNOS\n",
        "# cv2_imshow(thresh, f\"Umbralización Binaria (Umbral = {thresh_value})\") #This line is changed\n",
        "cv2_imshow(thresh) #updated call to cv2_imshow\n",
        "print(f\"Umbralización Binaria (Umbral = {thresh_value})\")\n",
        "contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "print(f\"Se encontraron {len(contours)} contornos.\")"
      ],
      "metadata": {
        "id": "E0ZF9-1ZmRom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Segmentación de contornos usando SIC\n",
        "\n",
        "# Cargar imagen\n",
        "image_path = '/content/roma.png'\n",
        "image = cv2.imread(image_path)\n",
        "image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "image_float = img_as_float(image)\n",
        "\n",
        "# @markdown Aplicamos SLIC a traves de super pixels\n",
        "segments = slic(image_float, n_segments=250, compactness=10, start_label=1)\n",
        "\n",
        "# @markdown Dibujar contornos con mark_boundaries\n",
        "output = mark_boundaries(image, segments, color=(0, 0, 0))\n",
        "img_contours = img.copy()\n",
        "# @title Dibujar los contornos\n",
        "cv2.drawContours(img_contours, contours, -1, (0, 255, 0), 2) # Color verde, grosor 2\n",
        "\n",
        "cv2_imshow(img_contours)\n"
      ],
      "metadata": {
        "id": "JK_qZI94m2ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### COMO SE OBSERVA EN ESTE CUERPO SE GENERA OTRO EJEMPLO QUE PERMITE OBSERVAR LA MISMA TECNICA PERO AHORA DANDO USO A LOS SUPERPIXELS"
      ],
      "metadata": {
        "id": "UMOs-Nott8fX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Mostrar resultado de segmentación tipo SLIC SUPERPIXEL para muestra de contornos por agrupamiento de la imagen\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.title('Segmentación SLIC')\n",
        "plt.imshow(output)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bSNeu1QLqKWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código carga una imagen y aplica el algoritmo SLIC para segmentarla en superpíxeles. Primero, convierte la imagen a RGB y luego a valores flotantes. SLIC agrupa píxeles similares en regiones compactas. Luego, mark_boundaries dibuja los bordes de estos superpíxeles en negro sobre la imagen original. Finalmente, intenta dibujar contornos detectados (aunque la detección real de contornos no está explícitamente en este fragmento) en verde sobre una copia de la imagen original, mostrando el resultado con cv2_imshow. El objetivo es visualizar la segmentación de la imagen mediante contornos."
      ],
      "metadata": {
        "id": "pJ6t9xxoqoMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title uso de mascaras de tipo SUPERPIXEL para reconocimiento colormetrico de objetos\n",
        "\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "image_float = img_as_float(image)\n",
        "\n",
        "# Aplicar SLIC\n",
        "segments = slic(image_float, n_segments=250, compactness=10, start_label=1)\n",
        "\n",
        "# Pintar regiones con colores aleatorios\n",
        "image_segments = label2rgb(segments, image, kind='avg')\n",
        "\n",
        "# @markdown Dibujar bordes sobre la imagen segmentada <-- COMO SE OBSERVA EN EL ANTERIOR mark_boundaries permite la especificación de contornos asimismo permitiendo la colorización\n",
        "output = mark_boundaries(image_segments, segments, color=(0, 0, 1))  # Borde azul\n",
        "\n",
        "# Mostrar resultado final\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.title('Segmentación SUPERPIXEL tipo máscara con bordes')\n",
        "plt.imshow(output)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CNa2B4YrnjFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Segmentación de imagen para obtención del fondo usando tecnias de color LAB y rembg\n",
        "\n",
        "Este código Python primero elimina el fondo de una imagen usando la librería rembg. Luego, convierte la imagen resultante a un formato manejable por OpenCV y extrae su canal alfa (transparencia) para crear una máscara del objeto.\n",
        "A continuación, aplica el algoritmo de clustering K-means sobre la imagen convertida al espacio de color LAB para segmentar los colores en dos grupos, identificando el fondo por su frecuencia. Se crea otra máscara basada en esta segmentación.\n",
        "Finalmente, combina ambas máscaras para aislar el objeto principal (sin fondo) que también pertenece al grupo de color predominante del objeto, mostrando los resultados."
      ],
      "metadata": {
        "id": "aGKESt0zvINm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rembg\n",
        "!pip install onnxruntime"
      ],
      "metadata": {
        "id": "c82O6KQ0uKeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui se presenta como la gestión de color usando los TERM CRITERIA Y color labs se pueden crear mascaras de condo para asi retirarlo\n",
        "image_path = '/content/calle.png'\n",
        "image = cv2.imread(image_path)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Redimensionar (opcional)\n",
        "scale_percent = 50  # Reducir tamaño\n",
        "width = int(image.shape[1] * scale_percent / 100)\n",
        "height = int(image.shape[0] * scale_percent / 100)\n",
        "image = cv2.resize(image, (width, height))\n",
        "\n",
        "# Convertir imagen a 2D para clustering\n",
        "pixel_values = image.reshape((-1, 3))\n",
        "pixel_values = np.float32(pixel_values)\n",
        "\n",
        "# Definir K-means\n",
        "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
        "k = 2  # Fondo vs Objeto\n",
        "_, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
        "\n",
        "# Convertimos a uint8\n",
        "centers = np.uint8(centers)\n",
        "segmented_image = centers[labels.flatten()]\n",
        "segmented_image = segmented_image.reshape(image.shape)\n",
        "\n",
        "# Crear máscara para retirar fondo\n",
        "labels = labels.flatten()\n",
        "background_label = np.argmax(np.bincount(labels))  # El cluster más grande es el fondo\n",
        "mask = np.where(labels == background_label, 0, 1)\n",
        "mask = mask.reshape((image.shape[0], image.shape[1]))\n",
        "\n",
        "# Aplicar máscara\n",
        "result = image * mask[:, :, np.newaxis]\n",
        "\n",
        "# Mostrar resultados\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title('Imagen Original')\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title('Segmentación')\n",
        "plt.imshow(segmented_image)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title('Objeto de fondo invertido')\n",
        "plt.imshow(result)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J15-4iTetG0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rembg import remove\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# @markdown Leer imagen original como bytes\n",
        "input_path = '/content/calle.png'\n",
        "with open(input_path, 'rb') as f:\n",
        "    input_image = f.read()\n",
        "\n",
        "# @markdown Elimina el fondo con rembg y convertir a PIL Image con transparencia (RGBA)\n",
        "output_image = remove(input_image)\n",
        "output_pil = Image.open(io.BytesIO(output_image)).convert('RGBA')\n",
        "\n",
        "# @markdown Convertierte PIL Image a array numpy y separar RGB y canal alfa\n",
        "image_rgba = np.array(output_pil)\n",
        "image_rgb = cv2.cvtColor(image_rgba[:, :, :3], cv2.COLOR_RGB2BGR)\n",
        "alpha = image_rgba[:, :, 3]\n",
        "\n",
        "# @markdown Crear máscara binaria donde el alfa > 0 es 1 (objeto) y 0 (fondo)\n",
        "mask_rembg = np.where(alpha > 0, 1, 0).astype(np.uint8)\n",
        "\n",
        "# @markdown Convertir imagen RGB a espacio de color LAB para mejor segmentación por color\n",
        "lab_image = cv2.cvtColor(image_rgb, cv2.COLOR_BGR2LAB)\n",
        "pixel_values = lab_image.reshape((-1, 3)).astype(np.float32)\n",
        "\n",
        "#  @markdown Aplicar K-means clustering para segmentar colores en 2 grupos\n",
        "k = 2\n",
        "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
        "_, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
        "\n",
        "labels = labels.flatten()\n",
        "#  @markdown Identificar la etiqueta del fondo encontrando el color más frecuente\n",
        "background_label = np.argmax(np.bincount(labels))\n",
        "\n",
        "# @markdown Crear máscara de segmentación donde la etiqueta del fondo es 0 y otras son 1\n",
        "mask_segment = np.where(labels == background_label, 0, 1).astype(np.uint8)\n",
        "mask_segment = mask_segment.reshape((image_rgb.shape[0], image_rgb.shape[1]))\n",
        "\n",
        "# @markdown Combinar las dos máscaras (rembg y segmentación): solo el objeto sin fondo Y segmentado\n",
        "final_mask = cv2.bitwise_and(mask_rembg, mask_segment)\n",
        "\n",
        "# @markdown Aplicar la máscara final a la imagen original para extraer el objeto segmentado\n",
        "result = cv2.bitwise_and(image_rgb, image_rgb, mask=final_mask)\n",
        "\n",
        "# Mostrar los resultados: imagen rembg, máscara de segmentación y objeto final\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 3, 1), plt.imshow(image_rgba), plt.title('Rembg'), plt.axis('off')\n",
        "plt.subplot(1, 3, 2), plt.imshow(mask_segment, cmap='gray'), plt.title('Segmentación LAB + KMeans'), plt.axis('off')\n",
        "plt.subplot(1, 3, 3), plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB)), plt.title('Objeto Final'), plt.axis('off')\n",
        "plt.tight_layout(), plt.show()"
      ],
      "metadata": {
        "id": "9CbR71UruOB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Pruebas de Detección de Anomalías y Segmentación Binaria utilizando el dataframe a traves del entrenamiento,  uso de tensorflow y su rendimiento frente a imágenes de ground truth. https://github.com/BIDS/BSDS500 y el AUTOENCODER CONVOLUCIONAL\n",
        "\n",
        "Este código implementa un autoencoder convolucional para la detección de anomalías en imágenes. Primero, load_images carga y preprocesa imágenes de un directorio. Luego, se define un autoencoder con una fase de codificación (Conv2D y MaxPooling) para reducir la dimensionalidad y una fase de decodificación (Conv2D y UpSampling2D) para reconstruir la imagen. El modelo se entrena para minimizar el error de reconstrucción en datos normales. Finalmente, detect_anomaly compara la imagen original con su reconstrucción para generar un mapa de anomalías y una máscara binaria, resaltando las regiones con mayores diferencias como posibles anomalías.\n",
        "\n",
        "Después de que el autoencoder reconstruye la imagen de entrada, se calcula la diferencia absoluta pixel a pixel entre la imagen original y la reconstruida. Luego, se crea un mapa de anomalías tomando la media de esta diferencia a través de los canales de color (RGB).\n",
        "\n",
        "La segmentación binaria se logra al aplicar un umbral (en este caso, 0.1) al mapa de anomalías. Cualquier valor en el mapa de anomalías que sea mayor que este umbral se considera parte de una posible anomalía y se establece en 1 (blanco) en la máscara de anomalías. Los valores por debajo del umbral se establecen en 0 (negro).\n",
        "\n",
        "El resultado es una máscara binaria donde las áreas blancas resaltan las regiones de la imagen original que el autoencoder no pudo reconstruir con precisión, lo que sugiere la presencia de anomalías.\n",
        "\n",
        "# Que es entonces el autoencoder convolucoinal\n",
        "\n",
        "Después de que el autoencoder reconstruye la imagen de entrada, se calcula la diferencia absoluta pixel a pixel entre la imagen original y la reconstruida. Luego, se crea un mapa de anomalías tomando la media de esta diferencia a través de los canales de color (RGB).\n",
        "\n",
        "La segmentación binaria se logra al aplicar un umbral (en este caso, 0.1) al mapa de anomalías. Cualquier valor en el mapa de anomalías que sea mayor que este umbral se considera parte de una posible anomalía y se establece en 1 (blanco) en la máscara de anomalías. Los valores por debajo del umbral se establecen en 0 (negro).\n",
        "\n",
        "El resultado es una máscara binaria donde las áreas blancas resaltan las regiones de la imagen original que el autoencoder no pudo reconstruir con precisión, lo que sugiere la presencia de anomalías.\n",
        "\n",
        "esta información se obtuvo de un curso que hice de UDEMY https://www.udemy.com/course/modern-computer-vision\n",
        "\n",
        "La verdad queria probar hace rato la ejecución de Autoencoders y pues gracias a este tutorial pude lograr identificar que estos tambien sirven para ralización de segmentación"
      ],
      "metadata": {
        "id": "u0u89gyGle0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-contrib-python --upgrade"
      ],
      "metadata": {
        "id": "ir49cMhBBXCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/BIDS/BSDS500.git"
      ],
      "metadata": {
        "id": "EwPPxaEegit0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"/content/BSDS500/BSDS500/data/images/\""
      ],
      "metadata": {
        "id": "fduK33Ynimoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "ZtPEix1-e3ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 3: Cargar imágenes y preparar datos\n",
        "def load_images(folder, size=(128, 128)):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img = cv2.imread(os.path.join(folder, filename))\n",
        "        if img is not None:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = cv2.resize(img, size)\n",
        "            img = img / 255.0\n",
        "            images.append(img)\n",
        "    return np.array(images)\n",
        "\n",
        "images = load_images(DATA_PATH + 'train')\n",
        "print(\"Total imágenes:\", images.shape)\n",
        "\n",
        "# Paso 4: Crear Autoencoder para detección de anomalías\n",
        "input_img = Input(shape=(128, 128, 3))\n",
        "\n",
        "# Encoder\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "# Decoder\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer=Adam(1e-3), loss='mse')\n",
        "autoencoder.summary()\n",
        "\n",
        "# Paso 5: Entrenar Autoencoder\n",
        "x_train, x_test = train_test_split(images, test_size=0.2, random_state=42)\n",
        "autoencoder.fit(x_train, x_train, epochs=30, batch_size=32, shuffle=True, validation_data=(x_test, x_test))\n",
        "\n",
        "# Paso 6: Detección de Anomalías y Segmentación Binaria\n",
        "def detect_anomaly(img):\n",
        "    pred = autoencoder.predict(img[np.newaxis, ...])[0]\n",
        "    diff = np.abs(img - pred)\n",
        "    anomaly_map = np.mean(diff, axis=2)  # Mapa de Anomalía\n",
        "    anomaly_mask = (anomaly_map > 0.1).astype(np.uint8)  # Segmentación Binaria\n",
        "    return pred, anomaly_map, anomaly_mask"
      ],
      "metadata": {
        "id": "gKMbQiwoextz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 7: Pruebas\n",
        "idx = np.random.randint(0, len(x_test))\n",
        "img = x_test[idx]\n",
        "\n",
        "deconstruida, mapa_anomalia, mascara = detect_anomaly(img)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.title(\"Imagen Original\")\n",
        "plt.imshow(img)\n",
        "\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.title(\"Deconstruccion\")\n",
        "plt.imshow(deconstruida)\n",
        "\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.title(\"Mapa Anomalía\")\n",
        "plt.imshow(mapa_anomalia, cmap='hot')\n",
        "\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.title(\"Segmentación\")\n",
        "plt.imshow(mascara, cmap='gray')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NZAO1Z3zkRhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total params: 13,347\n",
        "Trainable params: 13,347\n",
        "Non-trainable params: 0\n",
        "\n",
        "La evaluación de performance en la detección de anomalías mediante un autoencoder se centra en cuantificar qué tan bien el modelo identifica y localiza las irregularidades en las imágenes. El *ground truth* o verdad fundamental son las anotaciones o etiquetas que indican la ubicación y extensión real de las anomalías en un conjunto de imágenes de prueba.\n",
        "\n",
        "Para evaluar el rendimiento, se comparan las máscaras de anomalía generadas por el autoencoder con las máscaras de *ground truth*. Métricas comunes incluyen la precisión (precision), el recall (exhaustividad), la puntuación F1 (equilibrio entre precisión y recall) y el IoU (Intersection over Union) para la segmentación.\n",
        "\n",
        "El código proporcionado implementa un autoencoder convolucional para la detección de anomalías. Los Pasos 3, 4 y 5 preparan los datos, definen la arquitectura del autoencoder y lo entrenan para reconstruir imágenes normales. El Paso 6 genera un mapa de anomalía basado en la diferencia entre la imagen original y su reconstrucción, y luego aplica un umbral para crear una máscara binaria de segmentación.\n",
        "\n",
        "Para evaluar el rendimiento frente al ground truth, se necesitarían cargar las imágenes de ground truth correspondientes a las imágenes de prueba. Luego, para cada imagen de prueba, se aplicaría la función detect_anomaly y se compararían la máscara de anomalía resultante (anomaly_mask) con su máscara de ground truth. Se calcularían las métricas mencionadas anteriormente sobre el conjunto de imágenes de prueba para obtener una evaluación global del rendimiento del modelo."
      ],
      "metadata": {
        "id": "HusFnalOzie_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BIBLIOGRAFÍA\n",
        "\n",
        "+ Burney, S. M. A., & Tariq, H. (2014). K-Means Cluster Analysis for Image Segmentation. International Journal of Computer Applications, 96(4), 1–8. https://doi.org/10.5120/16779-6360\n",
        "\n",
        "+ Sammouda, R., & El-Zaart, A. (2021). An Optimized Approach for Prostate Image Segmentation Using K-Means Clustering Algorithm with Elbow Method. Computational Intelligence and Neuroscience, 2021, 1–10. https://doi.org/10.1155/2021/4553832\n",
        "\n",
        "+ Datanovia. (n.d.). Determining the optimal number of clusters: 3 must-know methods. Retrieved from https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/\n",
        "\n",
        "+ Winland, V. (n.d.). K-means clustering: Definition, applications, use cases. IBM. Retrieved from https://www.ibm.com/think/topics/k-means-clustering\n",
        "\n",
        "+ Chauhan, N. S. (2019, August 9). Introduction to Image Segmentation with K-Means clustering. KDnuggets. Retrieved from https://www.kdnuggets.com/2019/08/introduction-image-segmentation-k-means-clustering.html\n",
        "\n",
        "+ Tomar, A. (2025, March 13). Elbow Method in K-Means Clustering: Definition and Steps. Built In. Retrieved from https://builtin.com/data-science/elbow-method\n",
        "\n",
        "+ Hamza, M. (2024, November 4). K-Means Clustering for Image Segmentation Using OpenCV in Python. Towards Singularity. Retrieved from https://medium.com/towardssingularity/k-means-clustering-for-image-segmentation-using-opencv-in-python-17178ce3d6f3\n",
        "\n",
        "+ Safak, A. N. (2024, December 18). How to Interpret Silhouette Plot for K-Means Clustering. Medium. Retrieved from https://medium.com/@favourphilic/how-to-interpret-silhouette-plot-for-k-means-clustering-414e144a17fe\n",
        "\n",
        "+ Mahmood, M. (2020, March 16). Text Clustering: Comparing TF-IDF, BERT, and SBERT Embeddings with K-Means Clustering. Python in Plain English. Retrieved from https://python.plainenglish.io/text-clustering-comparing-tf-idf-bert-and-sbert-embeddings-with-k-means-clustering-a965fe7a69cc\n",
        "\n",
        "+ First Principles of Computer Vision. (n.d.). K-Means Segmentation | Image Segmentation. YouTube.(https://www.youtube.com/watch?v=BGBrK0d_nug)\n",
        "\n",
        "+ First Principles of Computer Vision. (n.d.). k-Means Segmentation | Image Segmentation. YouTube. https://www.youtube.com/watch?v=10cD46dQlgU\n",
        "\n",
        "+ Simplilearn. (2015). The Elbow Method for K-Means Clustering. YouTube. https://www.youtube.com/watch?v=ht7geyMAFfA\n",
        "\n",
        "+ Simplilearn. (2014). K-Means Clustering. YouTube. https://www.youtube.com/watch?v=22mpExWh1LY\n",
        "\n",
        "+ iddiqui, F. U., & Yahya, A. (2022). Clustering Techniques for Image Segmentation. Springer.\n",
        "\n",
        "+ Shovon, M. H. I. (2012). Clustering and Image Segmentation: Determining the number of cluster and Color Image Segmentation using K means. LAP LAMBERT Academic Publishing.\n"
      ],
      "metadata": {
        "id": "N2-yHUTE8-Ku"
      }
    }
  ]
}